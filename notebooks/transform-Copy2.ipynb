{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T01:51:13.647266Z",
     "start_time": "2020-04-16T01:51:12.383531Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup \n",
    "import pandas as pd\n",
    "import pymongo \n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pantab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T01:51:13.653765Z",
     "start_time": "2020-04-16T01:51:13.649335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Mongo \n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client.thousand_tabs\n",
    "collection = db.job_detail\n",
    "job_links_collection = db.job_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T01:51:14.011912Z",
     "start_time": "2020-04-16T01:51:13.960565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the URL links into a DataFrame\n",
    "url_links = pd.DataFrame(list(job_links_collection.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T01:51:17.373072Z",
     "start_time": "2020-04-16T01:51:14.692623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from our job detail collection in our raw_html_db database\n",
    "data = pd.DataFrame(list(collection.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T01:51:17.386012Z",
     "start_time": "2020-04-16T01:51:17.374392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Skill key words to compare to job descriptions\n",
    "skill_net = pd.read_csv('../data/skill_list.csv')\n",
    "skill_net_unique = pd.DataFrame(list(dict.fromkeys([x.strip().lower() for x in skill_net['skill_name']])), columns=['skill_net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T01:52:40.529845Z",
     "start_time": "2020-04-16T01:51:18.968499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of Beautiful Soup objects to iterate\n",
    "link_list = []\n",
    "for html in data.link_src:\n",
    "    link_list.append(soup(html,'html.parser'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T01:53:35.351071Z",
     "start_time": "2020-04-16T01:52:40.531632Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through each sublist of html, parse it, save the result in list. \n",
    "# then later convert to a dataframe\n",
    "headers = []\n",
    "job_titles = []\n",
    "senority = []\n",
    "posted = []\n",
    "employment_type = []\n",
    "job_function = []\n",
    "industry = []\n",
    "job_summary = []\n",
    "job_skills = []\n",
    "\n",
    "# Loop through HTMLs in URL link list from MongoDB\n",
    "for idx in range(len(link_list)):  \n",
    "    \n",
    "    # Extract Company Name and Company Location\n",
    "    for x in link_list[idx].find_all('h3','jobs-top-card__company-info t-14 mt1'):\n",
    "        vitals = x.get_text().replace('Company Name','').replace('Company Location','').split('\\n')\n",
    "        vitals = tuple([elem for elem in vitals if elem.strip()])\n",
    "        headers.append({'company':vitals[0].strip(),'location':vitals[1].strip()})\n",
    "    \n",
    "    # Extract Job Titles\n",
    "    try:\n",
    "        for title in link_list[idx].find('h1',\"jobs-top-card__job-title t-24\"):\n",
    "            job_titles.append(title)\n",
    "    except:\n",
    "        job_titles.append(None)\n",
    "    \n",
    "    # Extract Job Seniority (i.e. Junior, Senior, Director,etc.)\n",
    "    try:\n",
    "        for x in link_list[idx].find('p','jobs-box__body js-formatted-exp-body'):\n",
    "            senority.append(x)\n",
    "    except:\n",
    "        senority.append(None)\n",
    "    \n",
    "    # Extract Job Posting Time (i.e. 1 Week Ago)\n",
    "    try:\n",
    "        for x in link_list[idx].find_all('p','mt1 full-width flex-grow-1 t-14 t-black--light'):\n",
    "            posted.append(x.find_all('span')[1].text)\n",
    "    except:\n",
    "        posted.append(None)\n",
    "\n",
    "    # Extract Employment Type (i.e. Full-Time, Contract, Internship, etc.)\n",
    "    try:\n",
    "        for x in link_list[idx].find('p','jobs-box__body js-formatted-employment-status-body'):\n",
    "            employment_type.append(x)\n",
    "    except:\n",
    "        employment_type.append(None)\n",
    "\n",
    "    # Extract Job Functions into seperate DataFrame (i.e. Accounting, IT, Finance, etc.)\n",
    "    try:\n",
    "        for x in link_list[idx].find_all('ul','jobs-box__list jobs-description-details__list js-formatted-job-functions-list'):\n",
    "            for y in (x.get_text().strip('').split('\\n')[1:-1]):\n",
    "                job_function.append(((idx, y)))\n",
    "    except:\n",
    "        print(f'Job Function failed at {idx}')\n",
    "    \n",
    "    # Extract the Industry into seperate DataFrame (i.e. Entertainment, Media Production) \n",
    "    try:\n",
    "        for x in link_list[idx].find_all('ul','jobs-box__list jobs-description-details__list js-formatted-industries-list'):\n",
    "            for y in (x.get_text().strip('').split('\\n')[1:-1]):\n",
    "                industry.append(((idx, y)))\n",
    "    except:\n",
    "         print(f'Job Industry failed at {idx}')\n",
    "            \n",
    "###JOB SKILLS###\n",
    "\n",
    "# Extract skills from job descriptions\n",
    "# Making all words lowercase for easy comparison with our skill_net draglist\n",
    "for idx in range(len(link_list)):\n",
    "    for txt in link_list[idx].find_all('article','jobs-description__container jobs-description__container--condensed'):\n",
    "        job_summary.append(txt.get_text().lower().replace('\\n', ' ').strip())\n",
    "\n",
    "\n",
    "# Add spaces behind and in front of words to find the exact word ('r' => ' r ')\n",
    "skill_net_unique.skill_net =  pd.DataFrame(skill_net_unique.skill_net.map(lambda x: f' {x} '))    \n",
    "\n",
    "# Filtering text data by job on words inside of our skills list\n",
    "for idx, job in enumerate(job_summary):\n",
    "    for skill in skill_net_unique['skill_net']:\n",
    "        if skill in job:\n",
    "            job_skills.append((idx, skill))\n",
    "\n",
    "# Create job table\n",
    "df1 = pd.DataFrame(headers)\n",
    "df2 = pd.DataFrame(job_titles,columns=['job_title'])\n",
    "df3 = pd.DataFrame(senority,columns=['experience_level'])\n",
    "df4 = pd.DataFrame(posted,columns=['posted_time'])\n",
    "df5 = pd.DataFrame(employment_type,columns=['employment_type'])\n",
    "job_df = pd.concat([df2,df1,df3,df4,df5],axis=1)\n",
    "job_df['scrape_time'] = datetime.datetime.now()\n",
    "job_df = job_df.reset_index().rename(columns={'index':'job_id'})\n",
    "\n",
    "# Create job function, industry and skills tables\n",
    "job_function_df = pd.DataFrame(job_function, columns=['job_id', 'job_function'])\n",
    "industry_df = pd.DataFrame(industry, columns=['job_id','industry'])\n",
    "skills_df = pd.DataFrame(job_skills, columns=['job_id', 'skill'])\n",
    "\n",
    "# Extract Linkedin job link url\n",
    "# Create job url link table\n",
    "links_df = url_links.reset_index().rename(columns={'index':'job_id'})\n",
    "links_df.drop(columns='_id',inplace=True)\n",
    "links_df['link'] = links_df['link'].map(lambda x: 'www.linkedin.com' + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T08:27:12.569651Z",
     "start_time": "2020-04-14T08:27:12.566612Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dict = {'job': job_df,\n",
    "           'job_function': job_function_df,\n",
    "           'industry': industry_df,\n",
    "           'skill': skills_df,\n",
    "           'url': links_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T08:27:13.691475Z",
     "start_time": "2020-04-14T08:27:12.931326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job\n",
      "job_function\n",
      "industry\n",
      "skill\n",
      "url\n"
     ]
    }
   ],
   "source": [
    "def hyper_extract(df_dict):\n",
    "    for key in df_dict:\n",
    "        print(key)\n",
    "        pantab.frame_to_hyper(df_dict[key], f\"../data/{key}.hyper\", table = f'{key}')\n",
    "hyper_extract(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pantab\n",
    "pantab.frame_to_hyper(job_skills_df, \"../data/job_skills.hyper\", table = 'job_skill data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['search_query'] = 'data scientist'\n",
    "dataset['search_location'] = 'greater los angeles area'\n",
    "test2 = dataset.drop_duplicates(subset=['job_title','company', 'location', 'posted_time'])\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = []\n",
    "# for job in range(len(link_list)):\n",
    "#     try:\n",
    "#         for detail in link_list[job].find_all('span','jobs-ppc-criteria__value'):\n",
    "#             tmp.append((job,detail.get_text()))\n",
    "#     except: \n",
    "#         print('no data here')\n",
    "#         pass\n",
    "# tmp\n",
    "\n",
    "# final_job_skills_index  = pd.DataFrame(tmp,columns=['job_id','skills'])\n",
    "# final_job_skills_index.skills = final_job_skills_index.skills.map(lambda x: x.strip())\n",
    "# final_job_skills_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = url_links.reset_index().rename(columns={'index':'job_id'})\n",
    "links_df.drop(columns='_id',inplace=True)\n",
    "links_df['link'] = links_df['link'].map(lambda x: 'www.linkedin.com' + x)\n",
    "links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester.drop(columns='_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester['link'] = tester['link'].map(lambda x: 'www.linkedin.com' + x)\n",
    "# tester = tester.merge(final_job_skills_index,on='job_id',how='left')\n",
    "# tester[tester.skills.isnull()]\n",
    "# final_job_skills_index.to_csv('../data/skills_indexed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester[tester.skills.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# link_list[1].find_all('li','jobs-box__list-item jobs-description-details__list-item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "# connection_string = 'postgres:postgres@localhost:5432/li_analysis'\n",
    "# engine = create_engine(f'postgresql://{connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df7.to_sql(name='industry', con=engine, if_exists='replace', index=False)\n",
    "# df6.to_sql(name='job_function', con=engine, if_exists='replace', index=False)\n",
    "# final_job_skills_index.to_sql(name='skill', con=engine, if_exists='replace', index=False)\n",
    "# test.to_sql(name='job', con=engine, if_exists='replace', index=False)\n",
    "# df7\n",
    "# test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = dataset.reset_index().rename(columns={\"index\": \"job_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2_df = pd.merge(test2.reset_index().rename(columns={\"index\": \"job_id\"}), final_job_skills_index, on=['job_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2_df.to_csv('../data/thousand_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_skills = pd.read_csv('../data/all_skills.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_merge = test.merge(all_skills,on='job_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_merge.to_csv('../data/master_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.to_csv('../data/jobs_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_merge.drop(columns='Unnamed: 0',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lnks.drop(columns='_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lnks['job_id'] = np.arange(len(lnks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_merge_links = master_merge.merge(lnks, on='job_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_merge|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_merge_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_merge_links.link = ['www.linkedin.com' + x for x in master_merge_links['link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_merge_links.to_csv('../data/master_merge_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:33:38.992890Z",
     "start_time": "2020-04-14T09:33:38.988964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/jobs/view/1797065172/?eBP=NotAvailableFromVoyagerAPI&recommendedFlavor=COMPANY_RECRUIT&refId=3f9ba05b-79e5-4ac9-90a7-d29855349eee&trk=d_flagship3_search_srp_jobs'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_links['link'].iloc[667]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:33:39.452382Z",
     "start_time": "2020-04-14T09:33:39.446081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_id                                         667\n",
       "job_title           Applied Scientist II - AMZ3410\n",
       "company                                      Amgen\n",
       "location                     Thousand Oaks, CA, US\n",
       "experience_level                  Mid-Senior level\n",
       "posted_time                     Posted 1 month ago\n",
       "employment_type                          Full-time\n",
       "scrape_time             2020-04-14 01:26:54.890756\n",
       "Name: 667, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df.iloc[667]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:45:08.777644Z",
     "start_time": "2020-04-16T03:45:08.772375Z"
    }
   },
   "outputs": [],
   "source": [
    "tst = []\n",
    "for idx in range(len(link_list)):\n",
    "    try:\n",
    "        for x in link_list[idx].find_all('meta'):\n",
    "            print(x['href'])\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:37:24.898420Z",
     "start_time": "2020-04-16T03:37:24.894683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:47:47.502134Z",
     "start_time": "2020-04-16T03:47:47.492759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'          Focus GTS\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list[idx].find('a','jobs-top-card__company-url ember-view',href=True).get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
