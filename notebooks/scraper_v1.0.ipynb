{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import time\n",
    "import random \n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "linkedinuser = os.environ.get('linkedinuser')\n",
    "linkedpass = os.environ.get('linkedpass')\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize mongo \n",
    "onn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client.raw_html_db\n",
    "# db = client.test\n",
    "collection = db.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alvintomlin@outlook.com'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkedinuser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a list of current user agents \n",
    "url = 'https://developers.whatismybrowser.com/useragents/explore/operating_system_name/windows/'\n",
    "\n",
    "def get_agent_list(link,pagenum):\n",
    "    \"\"\"\n",
    "       Use this function to scape and generate a list of windows user agent strings.\n",
    "       Page num is calculated by visiting the url provided to this function and checking\n",
    "       to see how many pages are availble to scrape.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for i in range(pagenum):\n",
    "            time.sleep(1)\n",
    "            r = requests.get(url)\n",
    "            if r.status_code == 200:\n",
    "                data = r.content\n",
    "                soup = BeautifulSoup(data,'html.parser') \n",
    "                anchors = [a.get_text() for a in (td.find('a') for td in soup.findAll('td')) if a]\n",
    "            return anchors\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose a random user agent from the scraped list\n",
    "def generate_user_agent(agent_list):\n",
    "    try:\n",
    "        num = random.randint(0,len(agent_list[:25]))\n",
    "        ua = agent_list[num]\n",
    "        opts = Options()\n",
    "        return ua\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This scraper parses and returns the raw html used to get the job links\n",
    "def login_scrape(uname,pw,agent):\n",
    "\n",
    "    try:\n",
    "        pages = []\n",
    "        offset = 25\n",
    "        opts = Options()\n",
    "        opts.add_argument(f\"user-agent={agent}\")\n",
    "        driver = webdriver.Chrome('../driver/chromedriver',options=opts)\n",
    "        driver.get('http://linkedin.com')\n",
    "        login = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[1]/div[1]/input')\n",
    "        time.sleep(random.randrange(1,3))\n",
    "        login.send_keys(uname)\n",
    "        password = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[1]/div[2]/input')\n",
    "        password.send_keys(pw)   \n",
    "        submit = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[2]/button')\n",
    "        submit.send_keys(Keys.ENTER)\n",
    "        time.sleep(random.randrange(3,8))\n",
    " \n",
    "        \n",
    "    # scrape the job cards from the job card page\n",
    "        for i in range(40):\n",
    "            if i == 0:\n",
    "                driver.get('https://www.linkedin.com/jobs/search/?geoId=90000049&keywords=data%20scientist&location=Los%20Angeles%20Metropolitan%20Area&sortBy=R')\n",
    "                action = ActionChains(driver)\n",
    "                action.send_keys(Keys.TAB * 60)\n",
    "                action.perform()\n",
    "                time.sleep(random.randrange(5,8))\n",
    "                text_contents = BeautifulSoup(driver.page_source)\n",
    "                job_html = {'job':[driver.page_source]}\n",
    "                collection.insert_one(job_html)\n",
    "                pages.extend(text_contents)\n",
    "            \n",
    "      \n",
    "            else:\n",
    "                driver.get(f'https://www.linkedin.com/jobs/search/?geoId=90000049&keywords=data%20scientist&location=Los%20Angeles%20Metropolitan%20Area&sortBy=R&start={offset}')\n",
    "#                 time.sleep(random.randrange(1,3))\n",
    "                action = ActionChains(driver)\n",
    "                action.send_keys(Keys.TAB * 60)\n",
    "                action.perform()\n",
    "                time.sleep(random.randrange(5,8))\n",
    "                text_contents = BeautifulSoup(driver.page_source)\n",
    "                job_html = {'job':[driver.page_source]}\n",
    "                collection.insert_one(job_html)\n",
    "                pages.extend(text_contents)\n",
    "                time.sleep(random.randrange(5,8))\n",
    "                offset += 25            \n",
    "        \n",
    "        return pages\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping user agents \n",
    "data = get_agent_list(url,11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "# grabbing a random user agent\n",
    "ua = generate_user_agent(data)\n",
    "ua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login and scrape job card links from linkedin\n",
    "jobs = login_scrape(uname=linkedinuser,pw=linkedpass,agent=ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up another mongo collection to store the raw html for the links\n",
    "link_collection = db.job_links\n",
    "#Parse the raw html and collect individual job links \n",
    "links = []\n",
    "titles = []\n",
    "for x in range(len(jobs)):\n",
    "    for a in jobs[x].find_all('a','job-card-search__link-wrapper js-focusable disabled ember-view', href=True):\n",
    "            links.append(a['href'])\n",
    "            titles.append(a.text.strip())\n",
    "            link_collection.insert_one({\"link\":a.text.strip()})\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This scraper is run second. It takes the links collected by the previous scrape, visits the links and pull the\n",
    "#  source code and saves it to a  collection called job_detail in our raw_html_db mongo database\n",
    "db = client.raw_html_db\n",
    "job_detail_html = db.job_detail\n",
    "def job_listings(uname,pw,agent,link_list):\n",
    "\n",
    "    try:\n",
    "        job_pages = []\n",
    "        opts = Options()\n",
    "        opts.add_argument(f\"user-agent={agent}\")\n",
    "        driver = webdriver.Chrome('../driver/chromedriver',options=opts)\n",
    "        driver.get('http://linkedin.com')\n",
    "        login = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[1]/div[1]/input')\n",
    "        time.sleep(random.randrange(3,8))\n",
    "        login.send_keys(uname)\n",
    "        password = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[1]/div[2]/input')\n",
    "        time.sleep(random.randrange(3,8))\n",
    "        password.send_keys(pw)   \n",
    "        time.sleep(random.randrange(1,8))\n",
    "        submit = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[2]/button')\n",
    "        submit.send_keys(Keys.ENTER)\n",
    "        time.sleep(random.randrange(3,8))\n",
    " \n",
    "        for link in link_list:\n",
    "            driver.get(f'https://www.linkedin.com{link}')\n",
    "            time.sleep(random.randrange(2,4))\n",
    "            link_src = {'link_src':[driver.page_source]}\n",
    "            job_detail_html.insert_one(link_src)\n",
    "            job_pages.extend(BeautifulSoup(driver.page_source))\n",
    "        \n",
    "        return job_pages\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_final = job_listings(uname=linkedinuser,pw=linkedpass,agent=ua,link_list=links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
