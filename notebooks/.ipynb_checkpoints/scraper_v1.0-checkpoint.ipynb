{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import string\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "import time\n",
    "import random \n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "linkedinuser = os.environ.get('linkedinuser')\n",
    "linkedpass = os.environ.get('linkedpass')\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client.raw_html_db\n",
    "collection = db.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alvintomlin@outlook.com'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkedinuser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a list of current user agents \n",
    "url = 'https://developers.whatismybrowser.com/useragents/explore/operating_system_name/windows/'\n",
    "\n",
    "def get_agent_list(link,pagenum):\n",
    "    \"\"\"\n",
    "       Use this function to scape and generate a list of windows user agent strings.\n",
    "       Page num is calculated by visiting the url provided to this function and checking\n",
    "       to see how many pages are availble to scrape.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for i in range(pagenum):\n",
    "            time.sleep(1)\n",
    "            r = requests.get(url)\n",
    "            if r.status_code == 200:\n",
    "                data = r.content\n",
    "                soup = BeautifulSoup(data,'html.parser') \n",
    "                anchors = [a.get_text() for a in (td.find('a') for td in soup.findAll('td')) if a]\n",
    "            return anchors\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_agent(agent_list):\n",
    "    try:\n",
    "        num = random.randint(0,len(agent_list))\n",
    "        ua = agent_list[num]\n",
    "        opts = Options()\n",
    "        return ua\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This scraper is run first. It parses and returns the raw html used to get the indivdual job links\n",
    "def login_scrape(uname,pw,agent):\n",
    "\n",
    "    try:\n",
    "        pages = []\n",
    "        offset = 25\n",
    "        opts = Options()\n",
    "        opts.add_argument(f\"user-agent={agent}\")\n",
    "        driver = webdriver.Chrome('../driver/chromedriver',options=opts)\n",
    "        driver.get('http://linkedin.com')\n",
    "        login = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[1]/div[1]/input')\n",
    "        time.sleep(random.randrange(1,3))\n",
    "        login.send_keys(uname)\n",
    "        password = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[1]/div[2]/input')\n",
    "        password.send_keys(pw)   \n",
    "        submit = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[2]/button')\n",
    "        submit.send_keys(Keys.ENTER)\n",
    "        time.sleep(random.randrange(3,8))\n",
    " \n",
    "        \n",
    "    # scrape the job cards from the inital jobs landing page\n",
    "        for i in range(40):\n",
    "            if i == 0:\n",
    "                driver.get('https://www.linkedin.com/jobs/search/?geoId=90000049&keywords=data%20scientist&location=Los%20Angeles%20Metropolitan%20Area&sortBy=R')\n",
    "                action = ActionChains(driver)\n",
    "                action.send_keys(Keys.TAB * 60)\n",
    "                action.perform()\n",
    "                time.sleep(random.randrange(5,8))\n",
    "                #APPEND HTML TO MONGO DATABASE\n",
    "                text_contents = BeautifulSoup(driver.page_source)\n",
    "                job_html = {'job':[driver.page_source]}\n",
    "                collection.insert_one(job_html)\n",
    "                pages.extend(text_contents)\n",
    "            \n",
    "        \n",
    "        # Get the acutal html for the individual job post    \n",
    "            else:\n",
    "                driver.get(f'https://www.linkedin.com/jobs/search/?geoId=90000049&keywords=data%20scientist&location=Los%20Angeles%20Metropolitan%20Area&sortBy=R&start={offset}')\n",
    "#                 time.sleep(random.randrange(1,3))\n",
    "                action = ActionChains(driver)\n",
    "                action.send_keys(Keys.TAB * 60)\n",
    "                action.perform()\n",
    "                time.sleep(random.randrange(5,8))\n",
    "                text_contents = BeautifulSoup(driver.page_source)\n",
    "                job_html = {'job':[driver.page_source]}\n",
    "                collection.insert_one(job_html)\n",
    "                pages.extend(text_contents)\n",
    "                time.sleep(random.randrange(5,8))\n",
    "                offset += 25            \n",
    "        \n",
    "        return pages\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping user agents \n",
    "data = get_agent_list(url,11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "# grabbing a random user agent\n",
    "ua = generate_user_agent(data)\n",
    "ua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login and scrape job card links from linkedin\n",
    "jobs = login_scrape(uname=linkedinuser,pw=linkedpass,agent=ua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_collection = db.job_links\n",
    "\n",
    "links = []\n",
    "titles = []\n",
    "for x in range(len(jobs)):\n",
    "    for a in jobs[x].find_all('a','job-card-search__link-wrapper js-focusable disabled ember-view', href=True):\n",
    "            links.append(a['href'])\n",
    "            titles.append(a.text.strip())\n",
    "            link_collection.insert_one({\"link\":a.text.strip()})\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employers = []\n",
    "# for x in range(len(jobs)):\n",
    "#     for a in jobs[x].find_all('a','job-card-search__company-name-link t-normal ember-view', href=True):\n",
    "#         employers.append(a.text.strip())\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations = []\n",
    "# for x in range(len(jobs)):\n",
    "#     for a in jobs[x].find_all('span',\"job-card-search__location artdeco-entity-lockup__caption ember-view\"):\n",
    "#         locations.append(a.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_dict = dict(zip(employers,titles))\n",
    "# len(merged_dict)\n",
    "# merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.raw_html_db\n",
    "job_detail_html = db.job_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def job_listings(uname,pw,agent,link_list):\n",
    "\n",
    "    try:\n",
    "        job_pages = []\n",
    "        opts = Options()\n",
    "        opts.add_argument(f\"user-agent={agent}\")\n",
    "        driver = webdriver.Chrome('../driver/chromedriver',options=opts)\n",
    "        driver.get('http://linkedin.com')\n",
    "        login = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[1]/div[1]/input')\n",
    "        time.sleep(random.randrange(3,8))\n",
    "        login.send_keys(uname)\n",
    "        password = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[1]/div[2]/input')\n",
    "        time.sleep(random.randrange(3,8))\n",
    "        password.send_keys(pw)   \n",
    "        time.sleep(random.randrange(1,8))\n",
    "        submit = driver.find_element_by_xpath('/html/body/nav/section[2]/form/div[2]/button')\n",
    "        submit.send_keys(Keys.ENTER)\n",
    "        time.sleep(random.randrange(3,8))\n",
    " \n",
    "        for link in link_list:\n",
    "            driver.get(f'https://www.linkedin.com{link}')\n",
    "            time.sleep(random.randrange(2,4))\n",
    "            link_src = {'link_src':[driver.page_source]}\n",
    "            job_detail_html.insert_one(link_src)\n",
    "            job_pages.extend(BeautifulSoup(driver.page_source))\n",
    "        \n",
    "        return job_pages\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_final = job_listings(uname=linkedinuser,pw=linkedpass,agent=ua,link_list=links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ember4 > div.application-outlet > div.authentication-outlet > section.job-search-ext.job-search-ext--two-pane > div.jobs-search-two-pane__wrapper.jobs-search-two-pane__wrapper--two-pane > div > div > div.jobs-search-two-pane__results.display-flex > div.jobs-search-results.jobs-search-results--is-two-pane > div > ula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs_final[1].find_all('span','jobs-ppc-criteria__value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details = []\n",
    "for job in range(len(jobs_final)):\n",
    "    for detail in jobs_final[job].find_all('span','jobs-ppc-criteria__value'):\n",
    "        job_details.append(detail.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = pd.DataFrame(job_details,\n",
    "                      columns=['skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = skills.apply(lambda x: x.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = skills.apply(lambda y: y.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "skills = skills.apply(lambda x: [lemmatizer.lemmatize(y) for y in x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(30,20))\n",
    "# ax.tick_params(labelsize=30)\n",
    "# skills.skills.str.split(expand=True,).stack().value_counts()[:20].sort_values(ascending=False).plot(kind='barh',ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(ngram_range=(2,4),min_df=.002,max_df=.98)\n",
    "term_mat = tvec.fit_transform(skills.skills)\n",
    "term_df = pd.DataFrame(term_mat.toarray(), columns = cvec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30,20))\n",
    "ax.tick_params(labelsize=45,pad=20)\n",
    "ax.set_title('Analyst Keywords Los Angeles',fontsize=40,pad=20)\n",
    "ax.set_xlabel('Keyword Count',fontsize=40,labelpad=20)\n",
    "ax.set_ylabel('Keywords',fontsize=30,labelpad=20)\n",
    "term_df.sum().sort_values(ascending=False)[:40].plot(kind='barh',width=.8,\n",
    "                                                    edgecolor='white')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills.to_csv('../data/skills_keyword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary = []\n",
    "for idx in range(len(jobs_final)):\n",
    "    for txt in jobs_final[idx].find_all('article','jobs-description__container jobs-description__container--condensed'):\n",
    "        job_summary.append([txt.get_text()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/job_summaries.pkl','wb') as f:\n",
    "#     pickle.dump(links,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary  = np.char.lower(job_summary)\n",
    "job_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = r'!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~'\n",
    "for i in symbols:\n",
    "    job_summary = np.char.replace(job_summary, i, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary = np.char.replace(job_summary, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary = pd.DataFrame(job_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary = job_summary.rename(columns={0:'job description'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "job_summary.apply(lambda x: [lemmatizer.lemmatize(y) for y in x]) \n",
    "job_summary = pd.DataFrame(job_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = SnowballStemmer(language='english') \n",
    "job_summary.apply(lambda x: [stem.stem(y) for y in x]) \n",
    "job_summary = pd.DataFrame(job_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary.apply(lambda x: [nltk.sent_tokenize(y) for y in x]) \n",
    "job_summary = pd.DataFrame(job_summary)\n",
    "# job_summary.to_csv('../data/job_summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_summary[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec1 = TfidfVectorizer(stop_words='english',min_df=.12,ngram_range=(2,2),\n",
    "                        max_df=.86,token_pattern='[a-zA-Z0-9]{4,}')\n",
    "term_mat1 = cvec1.fit_transform(job_summary['job description'])\n",
    "term_df1 = pd.DataFrame(term_mat1.toarray(), columns = cvec1.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [5,8]\n",
    "                }\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(learning_method='online',\n",
    "                                learning_offset=600,learning_decay=.85,\n",
    "                                max_iter=350)\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda,param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(term_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(term_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(40,35))\n",
    "# ax.tick_params(labelsize=45,pad=20)\n",
    "# ax.set_title('Job Description Keywords Los Angeles',fontsize=50,pad=20)\n",
    "# ax.set_xlabel('Keyword Count',fontsize=50,labelpad=20)\n",
    "# ax.set_ylabel('Keywords',fontsize=60,labelpad=20)\n",
    "# term_df1.sum().sort_values(ascending=False)[:40].plot(kind='barh',width=.8,\n",
    "#                                                     edgecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LDA = LatentDirichletAllocation(n_components=10,random_state=42)\n",
    "# LDA.fit_transform(term_df1)\n",
    "\n",
    "# names = cvec1.get_feature_names()\n",
    "# names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(cvec1.get_feature_names()))\n",
    "    print(cvec1.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_topic = LDA.components_[0]\n",
    "\n",
    "top_topic_words = first_topic.argsort()[-10:]\n",
    "for i in top_topic_words:\n",
    "    print(cvec1.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([cvec1.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_values = LDA.transform(term_mat1)\n",
    "topic_values.shape\n",
    "job_summary['Topic'] = topic_values.argmax(axis=1)\n",
    "job_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, term_mat1, cvec1, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    " \n",
    "# # just send in all your docs here\n",
    "# tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(term_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    " \n",
    "# # place tf-idf values in a pandas data frame\n",
    "# df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "# df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tvec = TfidfVectorizer(stop_words='english',\n",
    "#                       min_df=0.40,norm='l2',max_df=.9)\n",
    "# vec_mat = tvec.fit_transform(job_summary['job description'])\n",
    "# term_df = pd.DataFrame(vec_mat.toarray(), columns = tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data = job_summary[['job description']].values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/job_links.pkl', 'rb') as f:\n",
    "    data1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
